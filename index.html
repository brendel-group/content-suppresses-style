<!doctype html>
<html lang="en">

<head>
    <!-- Required meta tags -->
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

    <!-- Primary Meta Tags -->
    <title>Content Suppresses Style: Dimensionality Collapse in Contrastive Learning</title>
    <meta name="title" content="Content Suppresses Style: Dimensionality Collapse in Contrastive Learning">
    <meta name="description"
        content="Contrastive learning is a highly successful yet simple self-supervised learning technique that minimizes the representational distance of similar (positive) while maximizing it for dissimilar (negative) samples. Despite its success, our theoretical understanding of contrastive learning is still incomplete. Most importantly, it is unclear why the inferred representation faces a dimensionality collapse after SimCLR training and why downstream performance improves by removing the feature encoder's last layers (projector). We show that collapse might be induced by an inductive bias of the InfoNCE loss for features that vary little within a positive pair (content) while suppressing more strongly-varying features (style). When at least one content variable is present, we prove that a low-rank projector reduces downstream task performance while simultaneously minimizing the InfoNCE objective. This result elucidates a potential reason why removing the projector could lead to better downstream performance. Subsequently, we propose a simple strategy leveraging adaptive temperature factors in the loss to equalize content and style latents, mitigating dimensionality collapse. Finally, we validate our theoretical findings on controlled synthetic data and natural images.">

    <!-- Open Graph / Facebook -->
    <meta property="og:type" content="website">
    <meta property="og:url" content="https://brendel-group.github.io/content-suppresses-styles/">
    <meta property="og:title" content="Content Suppresses Style: Dimensionality Collapse in Contrastive Learning">
    <meta property="og:description"
        content="Contrastive learning is a highly successful yet simple self-supervised learning technique that minimizes the representational distance of similar (positive) while maximizing it for dissimilar (negative) samples. Despite its success, our theoretical understanding of contrastive learning is still incomplete. Most importantly, it is unclear why the inferred representation faces a dimensionality collapse after SimCLR training and why downstream performance improves by removing the feature encoder's last layers (projector). We show that collapse might be induced by an inductive bias of the InfoNCE loss for features that vary little within a positive pair (content) while suppressing more strongly-varying features (style). When at least one content variable is present, we prove that a low-rank projector reduces downstream task performance while simultaneously minimizing the InfoNCE objective. This result elucidates a potential reason why removing the projector could lead to better downstream performance. Subsequently, we propose a simple strategy leveraging adaptive temperature factors in the loss to equalize content and style latents, mitigating dimensionality collapse. Finally, we validate our theoretical findings on controlled synthetic data and natural images.">
    <meta property="og:image" content="">

    <!-- Twitter -->
    <meta property="twitter:card" content="summary_large_image">
    <meta property="twitter:url" content="https://brendel-group.github.io/content-suppresses-styles/">
    <meta property="twitter:title" content="Content Suppresses Style: Dimensionality Collapse in Contrastive Learning">
    <meta property="twitter:description"
        content="Contrastive learning is a highly successful yet simple self-supervised learning technique that minimizes the representational distance of similar (positive) while maximizing it for dissimilar (negative) samples. Despite its success, our theoretical understanding of contrastive learning is still incomplete. Most importantly, it is unclear why the inferred representation faces a dimensionality collapse after SimCLR training and why downstream performance improves by removing the feature encoder's last layers (projector). We show that collapse might be induced by an inductive bias of the InfoNCE loss for features that vary little within a positive pair (content) while suppressing more strongly-varying features (style). When at least one content variable is present, we prove that a low-rank projector reduces downstream task performance while simultaneously minimizing the InfoNCE objective. This result elucidates a potential reason why removing the projector could lead to better downstream performance. Subsequently, we propose a simple strategy leveraging adaptive temperature factors in the loss to equalize content and style latents, mitigating dimensionality collapse. Finally, we validate our theoretical findings on controlled synthetic data and natural images.">
    <meta property="twitter:image" content="">

    <!-- Bootstrap CSS -->
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.2.2/dist/css/bootstrap.min.css" rel="stylesheet"
    integrity="sha384-Zenh87qX5JnK2Jl0vWa8Ck2rdkQ2Bzep5IDxbcnCeuOxjzrPF/et3URy9Bv1WTRi" crossorigin="anonymous">

    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

    <link href="https://fonts.googleapis.com/css2?family=IBM+Plex+Sans+Condensed&display=swap" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css2?family=IBM+Plex+Mono&display=swap" rel="stylesheet">

    <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.13.1/css/all.min.css" rel="stylesheet">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/Chart.js/2.9.3/Chart.min.js"></script>

    <style>
        .main {
            font-family: 'IBM Plex Sans Condensed', sans-serif;
        }

        .code {
            font-family: 'IBM Plex Mono', monospace;
        }

        .row {
            padding-top: 20px;
        }

        .row-dense {
            padding-top: 0;
        }

        .a {
            color: gainsboro;
            font-family: 'IBM Plex Sans Condensed', sans-serif;
        }

        a {
            text-decoration: none;
        }

        td {
            padding: 0 15px;
        }

        p {
            text-align: justify;

        }

        .collapse-container {
            text-align: center;
            position: relative;

        }

        .collapse-container #moreless.collapsed:after {
            content: '+ Show More';
        }

        .collapse-container #moreless:not(.collapsed):after {
            content: '- Show Less';
        }

        .collapse-container .collapse.collapse:not(.show) {
            display: block;
            /* height = lineheight * no of lines to display */
            height: 7.7em;
            overflow: hidden;
        }

        .collapse-container .collapse.collapse:not(.show):before {
            content: '';
            width: 100%;
            height: 7.7em;
            position: absolute;
            left: 0;
            top: 0;
            background: linear-gradient(rgba(255, 255, 255, 0), 60px, white);
        }

        .collapse-container .collapse.collapsing {
            height: 7.7em;
        }

        .badge-primary {
            color: #fff;
            background-color: #007bff;
        }
    </style>

    <title>Content Suppresses Style: Dimensionality Collapse in Contrastive Learning</title>
</head>

<body>
    <div class="container main">
        <div class="row">
            <div class="col-sm-2">
            </div>
            <div class="col-sm-8" id="main-content">
                <div class="row text-center my-5" id="#">
                    <h1>Content Suppresses Style: Dimensionality Collapse in Contrastive Learning</h1>
                </div>

                <!-- Begin author list-->
                <div class="row text-center mb-4">
                    <div class="col-sm-4 mb-4">
                        Evgenia Rusak*
                        <a href="mailto:evegnia.rusak@uni-tuebingen.de"><i class="far fa-envelope"></i></a></br>
                        University of Tübingen & <nobr>IMPRS-IS</nobr>
                    </div>
                    <div class="col-sm-4 mb-4">
                        Patrik Reizinger*
                        <a href="mailto:patrik.reizinger@uni-tuebingen.de"><i class="far fa-envelope"></i></a></br>
                        University of Tübingen, <nobr>IMPRS-IS & ELLIS</nobr>
                    </div>
                    <div class="col-sm-4 mb-4">
                        Roland S. Zimmermann*
                        <a href="mailto:research@rzimmermann.com"><i class="far fa-envelope"></i></a>
                        <a href="https://rzimmermann.com" target="_blank"><i class="fas fa-link"></i></a></br>
                        University of Tübingen & <nobr>IMPRS-IS</nobr>
                    </div>
                    <div class="col-sm-6 mb-4">
                        Oliver Bringmann<br>
                        University of Tübingen
                    </div>
                    <div class="col-sm-6 mb-4">
                        Wieland Brendel<br>
                        University of Tübingen
                    </div>
                </div>
                <!-- End author list-->

                <div class="row text-center">
                    <div class="col-sm-6 mb-6">
                        <h4>
                            <a href="https://sslneurips22.github.io/paper_pdfs/paper_27.pdf" target="_blank">
                                <i class="fas fa-file-alt"></i>
                                Workshop Paper
                            </a>
                        </h4>
                    </div>
                    <div class="col-sm-6 mb-6">
                        <h4>
                            <a href="" target="_blank">
                                <i class="fab fa-github"></i>
                                Code (coming soon)
                            </a>
                        </h4>
                    </div>
                </div>

                <div class="row text-center">
                    <p>
                        <b>tl;dr:</b>
                        <span class="text-muted">
                            We analyze why contrastive learning with an InfoNCE objective leads to dimensionality collapse, zoom into one possible failure mode and demonstrate ways to mitigate this issue without sacrificing the learned projector.
                        </span>
                    </p>
                </div>

                <div class="row mt-2">
                    <h3>News</h3>
                </div>

                <div class="row">
                    <table>
                        <tr>
                            <td>
                                <span class="badge badge-pill badge-primary">Oct '22</span>
                            </td>
                            <td>
                                Our <a href="https://sslneurips22.github.io/paper_pdfs/paper_27.pdf">work</a> was accepted at the <a href="https://sslneurips22.github.io/"
                                    target="_blank">NeurIPS 2022 Workshop: Self-Supervised Learning - Theory and Practice</a>.
                            </td>
                        </tr>
                    </table>
                </div>

                <div class="row mt-2">
                    <div class="col-12">
                        <p>
                        </p>
                    </div>
                </div>

                <div class="row mt-2">
                    <h3>Abstract</h3>
                </div>
                <div class="row mt-2">
                    <div class="col-12 collapse-container">
                        <p class="collapse" id="abstractText" aria-expanded="false">
                            Contrastive learning is a highly successful yet simple self-supervised learning technique that minimizes the representational distance of similar (positive) while maximizing it for dissimilar (negative) samples. Despite its success, our theoretical understanding of contrastive learning is still incomplete. Most importantly, it is unclear why the inferred representation faces a dimensionality collapse after SimCLR training and why downstream performance improves by removing the feature encoder's last layers (projector). We show that collapse might be induced by an inductive bias of the InfoNCE loss for features that vary little within a positive pair (content) while suppressing more strongly-varying features (style). When at least one content variable is present, we prove that a low-rank projector reduces downstream task performance while simultaneously minimizing the InfoNCE objective.
                            This result elucidates a potential reason why removing the projector could lead to better downstream performance.
                            Subsequently, we propose a simple strategy leveraging adaptive temperature factors in the loss to equalize content and style latents, mitigating dimensionality collapse. Finally, we validate our theoretical findings on controlled synthetic data and natural images.
                        </p>
                       <a role="button" id="moreless" class="collapsed" data-toggle="collapse" href="#abstractText" aria-expanded="false" aria-controls="abstractText"></a>
                    </div>
                </div>

                <div class="row mt-2">
                    <h3>Dimensionality Collapse in Contrastive Learning</h3>
                </div>
                <div class="row mt-2 row-dense">
                    <div class="col-12">
                        <p>
                           Contrastive learning is a learning paradigm in which similar samples (positive pairs, e.g., augmentations of the same image)
                           are contrasted against dissimilar ones (negative pairs, i.e., different images). While this scheme has been demonstrated to work
                           well - and there even exists theory (partially) explaining its success - there remains one curiosity about it: Usually, the last layer(s)
                           of the trained network need(s) to be removed before using the remaining network as a feature encoder, as otherwise, the representations are not
                           expressive enough but are collapsed to a low-dimensional subspace. This phenomenon is known as the dimensionality collapse of contrastive learning.
                        </p>
                    </div>
                </div>
                <div class="row mt-2">
                    <div class="col-12">
                        <div style="text-align: center;">
                            <img src="img/Figure_1.svg" style="max-width: 800px;" />
                        </div>
                        <small class="text-muted">
                            If all ground-truth latent factors of positive pairs change, there exist theoretical guarantees that all latent factors can be recovered.
                            However, this setting is unlikely to occur in practice as some factors (<it>content</it>) might stay unchanged while others (<it>style</it>)
                            vary. In this scenario, there is no guarantee that all latent factors can be recovered. In fact, we show that the commonly
                            used InfoNCE objective induces a bias towards solutions that discard style information resulting in a dimensionality collapse.
                            Finally, we present simple techniques to mitigate this issue and reduce the dimensionality collapse.
                        </small>
                    </div>
                </div>

                <div class="row mt-2">
                    <h3>Explaining &amp; Mitigating the Dimensionality Collapse</h3>
                </div>
                <div class="row mt-2 row-dense">
                    <div class="col-12">
                        <p>
                           We present a theoretical analysis of what can lead to a dimensionality collapse and use this to inform two experimental
                           modifications to mitigate this issue. For one, we suggest to remove the normalization usually used in the InfoNCE objective,
                           for another, we introduce different temperature values for different latent dimensions in the loss. In controlled experiments
                           in which we have access to the ground-truth latent factors, we can confirm that these steps substantially reduce the dimensionality
                           collapse.
                        </p>
                    </div>
                </div>
                <div class="col-12" style="padding-left: 0; padding-right: 0">
                    <div style="text-align: center;">
                        <img src="img/Figure_2.svg" style="max-width: 800px;" />
                    </div>
                    <small class="text-muted">
                        Normalized singular values of the inferred latents’ spectrum of a linear model for different
                        scenarios in which more and more dimensions (increasing \(d_c\)) become content dimensions, 
                        and/or temperature values \(\tau\) of the InfoNCE objective are changed. For non-linear models, we use the information content (measured by an R² score)
                        drops (for style dimensions) in later layers - this can again be prevented by using different temperature values in the objective. 
                    </small>
                </div>

                <div class="row mt-2">
                    <h3>Reducing the Dimensionality Collapse on CIFAR-10</h3>
                </div>
                <div class="row mt-2 row-dense">
                    <div class="col-12">
                        <p>
                            On CIFAR10 the ground-truth latents are unknown. Thus, we can only reason about recovered or collapsed latents indirectly: by characterizing the latent covariance spectrum or by performing a linear readout on top of the latents.

                            We first remove the commonly used normalization in the InfoNCE objective. Next, following the intuition that the distributions generating the latent factors in CIFAR10 have different variances, we set different temperature values for different dimensions in the objective. This leads to an overall improved readout performance both before and after the projector and reduces the gap between the readout performances before and after the projector. Further, the spectrum indicates a substantially reduced dimensionality collapse.
                        </p>
                    </div>
                </div>

                <div class="col-12">
                    <div style="text-align: center;">
                        <img src="img/Figure_3.svg" style="max-width: 800px;" />
                    </div>
                    <small class="text-muted">
                        (a) Linear readout accuracy on frozen features when using representations before (solid) or after the projection head (dashed). We find that our method significantly reduces the gap between the two showcasing an alleviation of dimensionality collapse. (b) The reduction of the dimensionality collapse is also visible in the spectrum of the inferred latents where we see more contributing singular values using our method.
                    </small>
                </div>

                <div class="row">
                    <h3>Acknowledgements & Funding</h3>
                </div>
                <div class="row row-dense mt-2">
                    <div class="col-12 collapse-container">
                        <p class="collapse" id="acknowledgmentsText" aria-expanded="false">
                            We thank Julian Bitterwolf and Steffen Schneider for valuable discussions. <br>
                            This work was supported by the German Federal Ministry of Education and Research (BMBF): <a href="https://tuebingen.ai" target="_blank">Tübingen AI Center</a>, FKZ: 01IS18039A & 01IS18039B, and by the Machine Learning Cluster of Excellence, EXC number 2064/1 – Project number 390727645. 
                            WB acknowledges financial support via an Emmy Noether Grant funded by the German Research Foundation (DFG) under grant no. BR 6382/1-1. 
                            The authors thank the <a href="https://imprs.is.mpg.de/" target="_blank">International Max Planck Research School for Intelligent Systems (IMPRS-IS)</a> for supporting EV, PR and RSZ.
                            PR acknowledges his membership in the <a href="https://ellis.eu/">European Laboratory for Learning and Intelligent Systems (ELLIS)</a> PhD program.
                        </p>
                       <a role="button" id="moreless" class="collapsed" data-toggle="collapse" href="#acknowledgmentsText" aria-expanded="false" aria-controls="acknowledgmentsText"></a>
                    </div>
                </div>
                <div class="row">
                    <h3>BibTeX</h3>
                </div>
                <div class="row">
                    <p>When citing our project, please use our pre-print:</p>
                </div>
                <div class="row justify-content-md-center">
                    <div class="col-sm-8 rounded p-3 m-2" style="background-color:lightgray;">
                        <small class="code">
                            @article{rusak2022content,<br>
                            &nbsp;&nbsp;author = { <br>
                            &nbsp;&nbsp;&nbsp;&nbsp;Rusak, Evgenia and<br>
                            &nbsp;&nbsp;&nbsp;&nbsp;Reizinger, Patrik and<br>
                            &nbsp;&nbsp;&nbsp;&nbsp;Zimmermann, Roland S. and<br>
                            &nbsp;&nbsp;&nbsp;&nbsp;Bringmann, Oliver and<br>
                            &nbsp;&nbsp;&nbsp;&nbsp;Brendel, Wieland and<br>
                            &nbsp;&nbsp;},<br>
                            &nbsp;&nbsp;title = {<br>
                            &nbsp;&nbsp;&nbsp;&nbsp;Content Suppresses Style in<br>
                            &nbsp;&nbsp;&nbsp;&nbsp;Dimensionality Collapse<br>
                            &nbsp;&nbsp;&nbsp;&nbsp;in Contrastive Learning<br>
                            &nbsp;&nbsp;},<br>
                            &nbsp;&nbsp;year = {2022},<br>
                            }
                        </small>
                    </div>
                </div>

                <div class="row">
                    <a href="#" class="ml-auto"><i class="fas fa-sort-up"></i></a>
                </div>

            </div>
        </div>

    </div>

    <!-- Optional JavaScript -->
    <!-- jQuery first, then Popper.js, then Bootstrap JS -->
    <script src="https://code.jquery.com/jquery-3.6.1.slim.min.js"
        integrity="sha384-MYL22lstpGhSa4+udJSGro5I+VfM13fdJfCbAzP9krCEoK5r2EDFdgTg2+DGXdj+"
        crossorigin="anonymous"></script>
    <script src="https://cdn.jsdelivr.net/npm/@popperjs/core@2.11.6/dist/umd/popper.min.js"
        integrity="sha384-oBqDVmMz9ATKxIep9tiCxS/Z9fNfEXiDAYTujMAeBAsjFuCZSmKbSSUnQlmh/jp3"
        crossorigin="anonymous"></script>
    <script src="https://stackpath.bootstrapcdn.com/bootstrap/4.5.0/js/bootstrap.min.js"
        integrity="sha384-OgVRvuATP1z7JjHLkuOU7Xw704+h835Lr+6QL9UvYjZE3Ipu6Tp75j7Bh/kR0JKI"
        crossorigin="anonymous"></script>

    <script>
        $(function () {
            $('[data-toggle="tooltip"]').tooltip()
        })
    </script>

</body>

</html>

</html>
